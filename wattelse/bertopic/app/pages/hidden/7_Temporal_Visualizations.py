#  Copyright (c) 2024, RTE (https://www.rte-france.com)
#  See AUTHORS.txt
#  SPDX-License-Identifier: MPL-2.0
#  This file is part of Wattelse, a NLP application suite.

import streamlit as st
import pandas as pd
import locale
from loguru import logger



from typing import List
import numpy as np
from sklearn.manifold import TSNE
import plotly.express as px
# from langchain_openai import ChatOpenAI
# from langchain_core.messages import HumanMessage, SystemMessage


from wattelse.bertopic.temporal_metrics_embedding import TempTopic


import plotly.graph_objects as go

from wattelse.bertopic.utils import TIMESTAMP_COLUMN, TEXT_COLUMN
from app_utils import plot_topics_over_time
from state_utils import restore_widget_state, register_widget, save_widget_state

from wattelse.bertopic.app.app_utils import (
    plot_2d_topics,
    plot_topics_over_time,
    compute_topics_over_time,
)

from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from typing import List, Union, Tuple, Dict
from tqdm import tqdm
from bertopic import BERTopic
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from scipy.sparse import lil_matrix
import itertools
import plotly.graph_objects as go
import plotly.express as px
import torch
import umap
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from loguru import logger

import plotly.express as px
import numpy as np
import plotly.graph_objs as go


def display_documents_on_click(clicked_point):
    if clicked_point:
        point = clicked_point['points'][0]
        topic_id = int(point['customdata'][0])
        timestamp = pd.to_datetime(point['customdata'][1], unit='D')  # Convert to datetime

        topic_data = temptopic.final_df[(temptopic.final_df['Topic'] == topic_id) & (temptopic.final_df['Timestamp'] == timestamp)]
        documents = topic_data['Document'].tolist()

        with st.expander(f"Documents for Topic {topic_id} at Timestamp {timestamp}"):
            for doc in documents:
                st.write(doc)


# Set locale to get French date names
locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')

# Wide layout
st.set_page_config(page_title="WattelseÂ® topic", layout="wide")

# Stop script if no model is trained
if "topic_model" not in st.session_state.keys():
    st.error("Train a model to explore different temporal visualizations.", icon="ðŸš¨")
    st.stop()

# Restore widget state
restore_widget_state()



# SIDEBAR Menu
with st.sidebar:
    st.header("TEMPTopic Parameters")
    register_widget("window_size")
    window_size = st.number_input("Window Size", min_value=2, value=2, step=1, key="window_size", on_change=save_widget_state)

    register_widget("k")
    k = st.number_input("Number of Nearest Embeddings (k)", min_value=1, value=1, step=1, key="k", on_change=save_widget_state)

    register_widget("double_agg")
    double_agg = st.checkbox("Double Aggregation", value=True, key="double_agg", on_change=save_widget_state)

    register_widget("evolution_tuning")
    evolution_tuning = st.checkbox("Evolution Tuning", value=True, key="evolution_tuning", on_change=save_widget_state)

    register_widget("global_tuning")
    global_tuning = st.checkbox("Global Tuning", value=False, key="global_tuning", on_change=save_widget_state)

    register_widget("doc_agg")
    doc_agg_options = ["mean", "max", "min"]
    doc_agg = st.selectbox("Document-level Aggregation", options=doc_agg_options, index=0, key="doc_agg", on_change=save_widget_state)

    register_widget("global_agg")
    global_agg_options = ["mean", "max", "min"]
    global_agg = st.selectbox("Global Aggregation", options=global_agg_options, index=1, key="global_agg", on_change=save_widget_state)

    register_widget("alpha")
    alpha = st.number_input("alpha", min_value=0.0, max_value=1.0, value=0.5, step=0.1, key="alpha", on_change=save_widget_state)





# Determine available time granularities based on data
min_date = st.session_state['timefiltered_df']['timestamp'].min()
max_date = st.session_state['timefiltered_df']['timestamp'].max()
time_diff = max_date - min_date

available_granularities = ["Day"]
if time_diff >= pd.Timedelta(weeks=1):
    available_granularities.append("Week")
if time_diff >= pd.Timedelta(days=30):
    available_granularities.append("Month")
if time_diff >= pd.Timedelta(days=365):
    available_granularities.append("Year")

# Time granularity selection
register_widget("granularity")
time_granularity = st.selectbox("Select time granularity", [""] + available_granularities, key="granularity", on_change=save_widget_state)



# TEMPTopic fitting
if time_granularity != "":
    # If any of these parameters changes between two consecutive interactions, re-fit TEMPtopic
    if "prev_granularity" not in st.session_state or st.session_state.prev_granularity != time_granularity \
    or st.session_state.prev_k != k  or st.session_state.prev_window_size != window_size or st.session_state.prev_double_agg != double_agg \
    or st.session_state.prev_doc_agg != doc_agg or st.session_state.prev_global_agg != global_agg or st.session_state.prev_alpha != alpha \
    or st.session_state.prev_evolution_tuning != evolution_tuning or st.session_state.prev_global_tuning != global_tuning:
        
        # Aggregate dataframe based on selected time granularity
        df = st.session_state['timefiltered_df'].copy()  # Create a copy to avoid modifying the original dataframe

        # Convert 'timestamp' column to datetime if it's not already
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        if time_granularity == "Day":
            df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d')
        elif time_granularity == "Week":
            df['timestamp'] = df['timestamp'].dt.strftime('%Y-%W')
        elif time_granularity == "Month":
            df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m')
        elif time_granularity == 'Year':
            df['timestamp'] = df['timestamp'].dt.strftime('%Y')

        # Group by timestamp and aggregate the text and index as lists
        aggregated_df = df.groupby('timestamp').agg({TEXT_COLUMN: list, 'index': list}).reset_index()

        # Extract indices from st.session_state["timefiltered_df"]["index"]
        indices = st.session_state["timefiltered_df"]["index"]

        # Extract docs using the indices
        docs = [st.session_state["split_df"][TEXT_COLUMN][i] for i in indices]

        # Create a dictionary mapping the original index to the aggregated timestamp
        index_to_timestamp = {}
        for timestamp, idx_sublist in zip(aggregated_df['timestamp'], aggregated_df['index']):
            for idx in idx_sublist:
                index_to_timestamp[idx] = timestamp

        # Extract the corresponding timestamps for each doc using the index_to_timestamp dictionary
        timestamps_repeated = [index_to_timestamp[idx] for idx in indices]
        
        # Update TempTopic with the new docs and timestamps
        temptopic = TempTopic(
            st.session_state['topic_model'],
            docs,
            st.session_state['embeddings'],
            st.session_state['token_embeddings'],
            st.session_state['token_strings'],
            timestamps_repeated,
            evolution_tuning=evolution_tuning,
            global_tuning=global_tuning
        )

        # Display a spinner while fitting TempTopic
        with st.spinner("Fitting TempTopic..."):
            temptopic._topics_over_time()

        # Store the fitted TempTopic object and its parameters in the session state
        st.session_state.temptopic = temptopic
        st.session_state.aggregated_df = aggregated_df
        st.session_state.prev_window_size = window_size
        st.session_state.prev_k = k
        st.session_state.prev_double_agg = double_agg
        st.session_state.prev_evolution_tuning = evolution_tuning
        st.session_state.prev_global_tuning = global_tuning
        st.session_state.prev_doc_agg = doc_agg
        st.session_state.prev_global_agg = global_agg
        st.session_state.prev_granularity = time_granularity
        st.session_state.prev_alpha = alpha

    else:
        # Use the previously fitted TEMPTopic and its parameters
        alpha = st.session_state.prev_alpha
        time_granularity = st.session_state.prev_granularity
        global_agg = st.session_state.prev_global_agg
        doc_agg = st.session_state.prev_doc_agg
        global_tuning = st.session_state.prev_global_tuning
        evolution_tuning = st.session_state.prev_evolution_tuning
        double_agg = st.session_state.prev_double_agg
        k = st.session_state.prev_k
        window_size = st.session_state.prev_window_size
        aggregated_df = st.session_state.aggregated_df
        temptopic = st.session_state.temptopic








# TEMPTopic Results
if time_granularity != "":
    # Display the dataframes and visualizations using the saved TempTopic object

    with st.expander("Topic Evolution Dataframe"):
        st.dataframe(temptopic.final_df[["Topic", "Words", "Document", "Frequency", "Timestamp"]].sort_values(by=['Topic', 'Timestamp'], ascending=[True, True]),
                    use_container_width=True)
        temptopic.final_df[["Topic", "Words", "Document", "Frequency", "Timestamp"]].sort_values(by=['Topic', 'Timestamp'], ascending=[True, True]).to_json('final_df_test.json')

    with st.expander("Topic Info Dataframe"):
        st.dataframe(temptopic.topic_model.get_topic_info(), use_container_width=True)

    with st.expander("Documents per Date Dataframe"):
        st.dataframe(aggregated_df, use_container_width=True)

    with st.expander("TempTopic Visualizations"):
        topics_to_show = st.multiselect("Topics to Show", options=list(temptopic.final_df["Topic"].unique()), default=None)

        st.header("Temporal Stability Metrics")
        smoothing_factor = st.slider("Smoothing Factor", min_value=0.0, max_value=1.0, value=0.2, step=0.1)

        temptopic.calculate_overall_topic_stability(window_size=window_size, k=k, double_agg=double_agg, doc_agg=doc_agg,
                                                    global_agg=global_agg,alpha=alpha)

        col1, col2 = st.columns(2)

        with col1:
            fig_temporal_stability = temptopic.plot_temporal_stability_metrics(metric="topic_stability", smoothing_factor=smoothing_factor, topics_to_show=topics_to_show)
            st.plotly_chart(fig_temporal_stability, use_container_width=True)

        with col2:
            fig_temporal_representation_stability = temptopic.plot_temporal_stability_metrics(metric="representation_stability", smoothing_factor=smoothing_factor, topics_to_show=topics_to_show)
            st.plotly_chart(fig_temporal_representation_stability, use_container_width=True)


        st.header("Overall Topic Stability")
        normalize_overall_stability = st.checkbox("Normalize", value=False)
        fig_overall_stability = temptopic.plot_overall_topic_stability(topics_to_show=topics_to_show, normalize=normalize_overall_stability)
        st.plotly_chart(fig_overall_stability, use_container_width=True)

        st.header("3D Topic Evolution Visualization")
        perplexity = st.number_input("T-SNE Perplexity", min_value=5.0, max_value=50.0, value=30.0, step=1.0)
        learning_rate = st.number_input("T-SNE Learning Rate", min_value=10.0, max_value=1000.0, value=200.0, step=10.0)
        metric = st.selectbox("T-SNE Metric", ["cosine", "euclidean", "manhattan"])
        color_palette = st.selectbox("Color Palette", ["Plotly", "D3", "Alphabet"])

        fig_topic_evolution = temptopic.plot_topic_evolution(granularity=time_granularity, perplexity=perplexity, color_palette=color_palette,topics_to_show=topics_to_show)
        st.plotly_chart(fig_topic_evolution, theme="streamlit", use_container_width=True)



with st.spinner("Computing topics over time..."):
    with st.expander("Popularity of topics over time"):
        if TIMESTAMP_COLUMN in st.session_state["timefiltered_df"].keys():
            st.write("## Popularity of topics over time")

            # Parameters
            st.text_input(
                "Topics list (format 1,12,52 or 1:20)",
                key="dynamic_topics_list",
                value="0:10",
            )

            st.number_input("nr_bins", min_value=1, value=10, key="nr_bins")

            st.session_state["topics_over_time"] = compute_topics_over_time(
                st.session_state["parameters"],
                st.session_state["topic_model"],
                st.session_state["timefiltered_df"],
                nr_bins=st.session_state["nr_bins"],
            )

            # Visualize
            st.plotly_chart(plot_topics_over_time(
                    st.session_state["topics_over_time"],
                    st.session_state["dynamic_topics_list"],
                    st.session_state["topic_model"],
                ), use_container_width=True)
            




