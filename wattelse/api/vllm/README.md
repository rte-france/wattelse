# vLLM

[vLLM](https://github.com/vllm-project/vllm) is an efficient LLM serving library. It allows you to deploy an LLM and serve it through an endpoint that follows the OpenAI API. This folder only contains a config file and a script to launch the service. Once launched, you can interact with the service using the OpenAI API, see [wattelse/api/openai](../openai).