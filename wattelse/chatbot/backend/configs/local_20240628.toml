# Local LLM RAGBackend configuration file

[retriever]
# Maximum number of extracts to return
top_n_extracts = 10
# Retrieval method, choose among "mmr", "similarity", "similarity_score_threshold", "bm25", "ensemble"
retrieval_method = "similarity_score_threshold"
# similarity threshold (applies only for the method "similarity_score_threshold")
similarity_threshold = 0.3
# Generate alternative questions and retrieve documents based on those questions
# NB. Experimental feature... (generated questions may be out of scope)
multi_query_mode = false

[generator]
# Type of LLM API: select the right environment variables (LEGACY_*, LOCAL_*, AZURE_WATTELSE_*)
openai_api_key = "$LOCAL_OPENAI_API_KEY"
openai_endpoint = "$LOCAL_OPENAI_ENDPOINT"
openai_default_model = "$LOCAL_OPENAI_DEFAULT_MODEL_NAME"
# Use recent memory in interactions
remember_recent_messages = true
# Generation temperature
temperature = 0.1
