######## NEWSLETTER CONFIG ########
[newsletter]
id = newsletter_decarbonation_from_last
title = Newsletter Décarbonation
top_n_topics = 5
top_n_docs = 5
output_format = html
# output_directory is a relative path - base_output_path/output_directory; the base_output_path is controlled by the application
output_directory = newsletter_rte
summarizer_class = bertrend.summary.GPTSummarizer
prompt_language = fr
# To automate the newsletters generation, e.g. “At 22:00 on every day-of-week from Monday through Friday.”
update_frequency = 15 2 * * 3
recipients = [ "jerome.picault@rte-france.com", "guillaume.grosjean@rte-france.com", "nicolas.bonnerot@rte-france.com", "jerome.dejaegher@rte-france.com", "nader.narcisse_externe@rte-france.com" ]
improve_topic_description = True
# 'document' to summarize top n documents independently or 'topic' to generate a single topic summary
summary_mode = topic
openai_model_name = gpt-4o

######## LEARNING STRATEGY ########
[learning_strategy]
# Choose among:
# - learn_from_scratch : uses all available data from feed to create the model
# - learn_from_last: uses only the last feed data to create the model
# - inference_only: do not retrain model; reuse existing bertopic model if available,
#   otherwise, fallback to learn_from_scratch for the first run
learning_strategy = learn_from_last
split_data_by_paragraphs = True
# model saving path (empty = do not save model)
bertopic_model_path = models/decarbonation_from_last

######## TOPIC MODEL CONFIG ########
[topic_model.embedding]
# embedding model to be used, authorized values:
# - dangvantuan/sentence-camembert-large
# - paraphrase-multilingual-MiniLM-L12-v2
# - sentence-transformers/all-mpnet-base-v2
model_name = dangvantuan/sentence-camembert-large

[topic_model.topics]
# Topics
nr_topics = 0
top_n_words = 5

[topic_model.umap]
# UMAP
n_neighbors = 15
n_components = 5
min_dist = 0.0
metric = cosine

[topic_model.hdbscan]
# HDBScan
min_cluster_size = 10
min_samples = 10
metric = euclidean
cluster_selection_method = eom
prediction_data = True

[topic_model.count_vectorizer]
# Count Vectorizer
stop_words = french
ngram_range = (2,2)
min_df = 2

[topic_model.c_tf_idf]
# c-TF-IDF
reduce_frequent_words = True

